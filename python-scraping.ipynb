{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코딩 전에.. 개념 적립!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling vs Scraping\n",
    "\n",
    "- **Crawling (혹은 Web Crawling)** : search engine(혹은 bot)이 웹페이지 내 링크들을 타고 다른 페이지들을 들어가는 것. (우리가 원하는 정보를 찾을 때까지 링크를 타고 들어가는 거겠지!)\n",
    "- **Scraping** : 내가 원하는 정보를 수집하는 행위. (꼭 web에서 이루어질 필요 없음)\n",
    "\n",
    "(ref: https://smartproxy.com/what-is-web-scraping/crawling-vs-scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그렇다면.. 이제 코딩 시작!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `request`는 url을 활용해 html 소스를 가져오는 역할을 함. <br>\n",
    "(html 소스: 인터넷 페이지는 모두 코드로 이루어져 있는데, 그 코드를 뜻함)\n",
    "- `beutifulsoup4`는 html 소스에서 필요한 데이터를 추출하는 함수를 제공해줌. <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "예를 들어, https://goldkim92.github.io/online-cv/ 의 html 소스를 살펴보는 방법 2가지를 보자!!\n",
    "- 인터넷 사이트에 들어가서 `F12`를 눌러보자.\n",
    "- 밑에 코드를 돌려보자.\n",
    "\n",
    "두 결과가 같음을 확인할 수 있다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://goldkim92.github.io/online-cv/'\n",
    "html = requests.get(url)\n",
    "print(html.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### html 소스를 볼 때 알아두면 유용한 용어 4가지만 소개해볼게! ###\n",
    "- **`tag`** : html 소스를 보면 꺽쇠 표시들이 많이 보이지! `< >` 요런거. 이걸 `tag`라고 해. <br>\n",
    "ex. < html >, < head >, < body >, < meta > ...\n",
    "- **`element`** : 시작 tag 부터 끝 tag 까지의 구성\n",
    "- **`attrubute`** : 시작 tag 안에 구체적인 속성을 정의\n",
    "- **`value`** : attribute의 값\n",
    "\n",
    "![](./html_basic.png)\n",
    "\n",
    "reference 보면 더 잘 이해가 될거야! <br>\n",
    "(ref: https://miaow-miaow.tistory.com/21)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이제 예시로 data를 추출해보자! ###\n",
    "- 다시, 앞의 html 소스를 보면 `<head>` 가 보일 거야. 우리는 head tag 안에(nested) 있는 meta tag 를 모두 모아서 뽑아보고,\n",
    "- 각 meta tag 에서 content attribute의 value를 뽑아볼게."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "metas = soup.head.find_all('meta')\n",
    "metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta in metas:\n",
    "    print(meta.get('content'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- html 소스의 h2 tag를 모두 뽑아보고 싶다면,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('h2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그렇다면, 재명이의 publication/preprint의 제목 리스트를 뽑아볼까?! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(attrs={'class':'job-title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in soup.find_all(attrs={'class':'job-title'}):\n",
    "    print(element.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제는 실전이다! ##\n",
    "### 당근 마켓의 인기 메물들의 이름/장소/가격/관심/채팅개수 를 scraping 해보자\n",
    "\n",
    "- 그 전에! 먼저 데이터들을 저장할 excel파일을 우선적으로 만들어 놓자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('carrot-market-daily-best.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Title', 'Region', 'Price', 'n_Love', 'n_Chat'])\n",
    "    writer.writeheader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번엔 https://www.daangn.com/hot_articles 여기에 있는 당근마켓 메물들을 긁어모아보자구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.daangn.com/hot_articles'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = soup.find_all(attrs={'class':'card-desc'})\n",
    "len(cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card in cards:\n",
    "    # title\n",
    "    title = card.find(attrs={'class':'card-title'}).get_text()\n",
    "    \n",
    "    # region\n",
    "    region = card.find(attrs={'class':'card-region-name'}).get_text()\n",
    "    region = ' '.join(region.split())\n",
    "    \n",
    "    # price\n",
    "    price = card.find(attrs={'class':'card-price'}).get_text()\n",
    "    price = ' '.join(price.split())\n",
    "    \n",
    "    # n_love & n_chat\n",
    "    counts = card.find(attrs={'class':'card-counts'}).get_text()\n",
    "    n_love = counts.split()[1]\n",
    "    n_chat = counts.split()[4]\n",
    "    \n",
    "    # save in excel file\n",
    "    with open('carrot-market-daily-best.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([title, region, price, n_love, n_chat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
